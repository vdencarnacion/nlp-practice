{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This aims to be a documented practice to NLP.\n",
    "\n",
    "For starters, I will be trying this: https://www.kaggle.com/aaron7sun/stocknews\n",
    "\n",
    "But then I am not really familiar with AUC and decided to follow the definition from http://fastml.com/what-you-wanted-to-know-about-auc/\n",
    "\n",
    "Now I need to be able process the words in Python (personal preference) and decided to use the 'nltk package'\n",
    "\n",
    "Here's how I set it up using the command line:\n",
    "```\n",
    "pip install nltk\n",
    "import nltk\n",
    "nltk.download() #choose 'all' as in All packages\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practice code\n",
    "import nltk\n",
    "from nltk.book import *\n",
    "\n",
    "import matplotlib\n",
    "\n",
    "sentence = \"\"\"At eight o'clock on Thursday morning Arthur didn't feel very good.\"\"\"\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "tokens\n",
    "text1\n",
    "print('text1.concordance(\\'monstrous\\')----------------')\n",
    "text1.concordance('monstrous')\n",
    "print('text1.similar(\\'monstrous\\')----------------')\n",
    "text1.similar('monstrous')\n",
    "print('text2.similar(\\'monstrous\\')----------------')\n",
    "text2.similar('monstrous')\n",
    "print('text2.common_contexts([\"monstrous\", \"very\")]\\')----------------')\n",
    "text2.common_contexts([\"monstrous\", \"very\"])\n",
    "print('text4.dispersion_plot([\"citizens\", \"democracy\", \"freedom\", \"duties\", \"America\"])----------------')\n",
    "text4.dispersion_plot([\"citizens\", \"democracy\", \"freedom\", \"duties\", \"America\"])\n",
    "# print('text4.generate()----------------')\n",
    "# text4.generate()\n",
    "print('len(text1)----------------')\n",
    "len(text1)\n",
    "print('sorted(set(text3))----------------')\n",
    "sorted(set(text3))\n",
    "print('frequency----------------')\n",
    "print('fdist1 = FreqDist(text1)----------------')\n",
    "fdist1 = FreqDist(text1)\n",
    "print(fdist1)\n",
    "print('vocabulary1 = fdist1.keys()----------------')\n",
    "vocabulary1 = fdist1.keys()\n",
    "print('fdist1[\\'whale\\']----------------')\n",
    "fdist1['whale']\n",
    "print('fdist1.plot(50, cumulative=True)----------------')\n",
    "fdist1.plot(50, cumulative=True)\n",
    "# fdist1.hapaxes()\n",
    "\n",
    "# ----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Sentiment analysis using Large Movie Review Dataset\n",
    "\n",
    "Data came from: \n",
    "@InProceedings{maas-EtAl:2011:ACL-HLT2011,\n",
    "  author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\n",
    "  title     = {Learning Word Vectors for Sentiment Analysis},\n",
    "  booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n",
    "  month     = {June},\n",
    "  year      = {2011},\n",
    "  address   = {Portland, Oregon, USA},\n",
    "  publisher = {Association for Computational Linguistics},\n",
    "  pages     = {142--150},\n",
    "  url       = {http://www.aclweb.org/anthology/P11-1015}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
